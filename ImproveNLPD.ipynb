{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.205252</td>\n",
       "      <td>0.249533</td>\n",
       "      <td>2.468264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.778450</td>\n",
       "      <td>1.915088</td>\n",
       "      <td>0.846388</td>\n",
       "      <td>0.330514</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.121032</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>0.031075</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>0.031257</td>\n",
       "      <td>0.885887</td>\n",
       "      <td>0.190676</td>\n",
       "      <td>0.464940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.617950</td>\n",
       "      <td>0.057612</td>\n",
       "      <td>0.270210</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367086</td>\n",
       "      <td>0.169956</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.319232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037268</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.229228</td>\n",
       "      <td>0.870521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>2.016196</td>\n",
       "      <td>0.590654</td>\n",
       "      <td>0.166361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.583478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.180211</td>\n",
       "      <td>0.228068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>0.596071</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394930</td>\n",
       "      <td>0.044031</td>\n",
       "      <td>0.314719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439345</td>\n",
       "      <td>0.899247</td>\n",
       "      <td>0.614806</td>\n",
       "      <td>0.573302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511731</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.218518</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.133643</td>\n",
       "      <td>0.191010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481389</td>\n",
       "      <td>1.538416</td>\n",
       "      <td>0.215585</td>\n",
       "      <td>1.294243</td>\n",
       "      <td>0.387299</td>\n",
       "      <td>0.058883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.713862</td>\n",
       "      <td>0.455414</td>\n",
       "      <td>1.520771</td>\n",
       "      <td>0.060180</td>\n",
       "      <td>0.772773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280808</td>\n",
       "      <td>0.460845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230883</td>\n",
       "      <td>1.454101</td>\n",
       "      <td>0.414433</td>\n",
       "      <td>0.295896</td>\n",
       "      <td>0.029659</td>\n",
       "      <td>0.081867</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.072701</td>\n",
       "      <td>0.811972</td>\n",
       "      <td>0.363433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>0.253925</td>\n",
       "      <td>0.578918</td>\n",
       "      <td>0.255666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167994</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219655</td>\n",
       "      <td>0.073615</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205858</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.603405</td>\n",
       "      <td>0.786571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6205</th>\n",
       "      <td>0.274169</td>\n",
       "      <td>0.345489</td>\n",
       "      <td>0.470278</td>\n",
       "      <td>0.258174</td>\n",
       "      <td>1.462303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.393906</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055088</td>\n",
       "      <td>0.381468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611305</td>\n",
       "      <td>1.447888</td>\n",
       "      <td>0.134379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6206</th>\n",
       "      <td>2.546535</td>\n",
       "      <td>0.786237</td>\n",
       "      <td>0.159672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043736</td>\n",
       "      <td>0.658212</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027105</td>\n",
       "      <td>0.164124</td>\n",
       "      <td>0.443468</td>\n",
       "      <td>0.297745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085021</td>\n",
       "      <td>2.660668</td>\n",
       "      <td>0.036462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6207</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218152</td>\n",
       "      <td>0.058052</td>\n",
       "      <td>0.381265</td>\n",
       "      <td>1.797887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882641</td>\n",
       "      <td>0.160167</td>\n",
       "      <td>0.198532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103092</td>\n",
       "      <td>0.558721</td>\n",
       "      <td>1.345153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.580165</td>\n",
       "      <td>0.413328</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6208</th>\n",
       "      <td>0.385554</td>\n",
       "      <td>0.326320</td>\n",
       "      <td>0.398470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079357</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>0.131762</td>\n",
       "      <td>0.678016</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.495105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550121</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>1.907560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481531</td>\n",
       "      <td>0.055896</td>\n",
       "      <td>0.050559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6209 rows Ã— 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.205252  0.249533  2.468264  0.000000  2.778450  1.915088  0.846388   \n",
       "1     1.617950  0.057612  0.270210  0.006480  0.004228  0.000000  0.367086   \n",
       "2     0.000000  0.000000  0.000000  0.008871  0.583478  0.000000  1.180211   \n",
       "3     0.000000  0.439345  0.899247  0.614806  0.573302  0.000000  0.511731   \n",
       "4     0.713862  0.455414  1.520771  0.060180  0.772773  0.000000  0.280808   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6204  0.253925  0.578918  0.255666  0.000000  0.109713  0.000000  0.167994   \n",
       "6205  0.274169  0.345489  0.470278  0.258174  1.462303  0.000000  1.393906   \n",
       "6206  2.546535  0.786237  0.159672  0.000000  0.105839  0.000000  0.043736   \n",
       "6207  0.000000  0.218152  0.058052  0.381265  1.797887  0.000000  0.882641   \n",
       "6208  0.385554  0.326320  0.398470  0.000000  0.079357  0.168380  0.131762   \n",
       "\n",
       "          7         8         9     ...      2038      2039      2040  \\\n",
       "0     0.330514  0.027987  0.000000  ...  0.008679  0.121032  0.008315   \n",
       "1     0.169956  0.005445  0.319232  ...  0.037268  0.361500  0.229228   \n",
       "2     0.228068  0.000000  0.072185  ...  0.000000  0.050236  0.596071   \n",
       "3     0.006354  0.218518  0.041050  ...  0.000000  1.133643  0.191010   \n",
       "4     0.460845  0.000000  0.070070  ...  0.230883  1.454101  0.414433   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6204  0.287934  0.015431  0.021831  ...  0.219655  0.073615  0.027513   \n",
       "6205  0.034041  0.000000  0.000000  ...  0.081641  0.000000  0.077804   \n",
       "6206  0.658212  0.006366  0.000000  ...  0.027105  0.164124  0.443468   \n",
       "6207  0.160167  0.198532  0.000000  ...  0.000000  0.103092  0.558721   \n",
       "6208  0.678016  0.006071  0.495105  ...  0.275013  0.000000  0.000000   \n",
       "\n",
       "          2041      2042      2043      2044      2045      2046      2047  \n",
       "0     0.031075  0.017995  0.016965  0.031257  0.885887  0.190676  0.464940  \n",
       "1     0.870521  0.000000  0.586490  0.010039  2.016196  0.590654  0.166361  \n",
       "2     0.007021  0.000000  0.040999  0.000000  0.394930  0.044031  0.314719  \n",
       "3     0.000000  0.481389  1.538416  0.215585  1.294243  0.387299  0.058883  \n",
       "4     0.295896  0.029659  0.081867  0.004096  0.072701  0.811972  0.363433  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6204  0.227227  0.028447  0.000000  0.205858  0.017773  0.603405  0.786571  \n",
       "6205  0.000000  0.055088  0.381468  0.000000  0.611305  1.447888  0.134379  \n",
       "6206  0.297745  0.000000  0.000000  0.000000  0.085021  2.660668  0.036462  \n",
       "6207  1.345153  0.000000  0.000000  0.000000  1.580165  0.413328  0.000000  \n",
       "6208  0.550121  0.079096  1.907560  0.000000  0.481531  0.055896  0.050559  \n",
       "\n",
       "[6209 rows x 2048 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Weights2 - 6th  layer/Uk_weights2_X_test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerXtest = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_X_test.csv',header=None).to_numpy()\n",
    "layerYtest = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_Y_test.csv',header=None).to_numpy()\n",
    "layerXtrain = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_X_validation.csv',header=None).to_numpy()\n",
    "layerYtrain = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_Y_validation.csv',header=None).to_numpy()\n",
    "smXtest = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_test.csv',header=None).to_numpy()\n",
    "smYtest = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_test.csv',header=None).to_numpy()\n",
    "smXtrain = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_validation.csv',header=None).to_numpy()\n",
    "smYtrain = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_validation.csv',header=None).to_numpy\n",
    "\n",
    "#we'll include the training data too...\n",
    "#smXtrain2 = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_train.csv',header=None).to_numpy()\n",
    "#smYtrain2 = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_train.csv',header=None).to_numpy()\n",
    "#smXtrain = np.r_[smXtrain,smXtrain2]\n",
    "#smYtrain = np.r_[smYtrain,smYtrain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPD(actual,predicted):\n",
    "    \"\"\"(normalised) Negative Log Predictive Density\n",
    "    Definition of Negative Log Predictive Density (NLPD):\n",
    "    $$L = -\\frac{1}{n} \\sum_{i=1}^n \\log p(y_i=t_i|\\mathbf{x}_i)$$\n",
    "    See http://mlg.eng.cam.ac.uk/pub/pdf/QuiRasSinetal06.pdf, page 13.\n",
    "    \"This loss penalizes both over and under-confident predictions.\"\n",
    "    but\n",
    "    \"The NLPD loss favours conservative models, that is models that tend to be under-confident\n",
    "    rather than over-confident. This is illustrated in Fig. 7, and canbe deduced from the fact that\n",
    "    logarithms are being used. An interesting way ofusing the NLPD is to give it relative to the NLPD\n",
    "    of a predictor that ignoresthe inputs and always predicts the same Gaussian predictive distribution,\n",
    "    withmean and variance the empirical mean and variance of the training data. Thisrelative NLPD\n",
    "    translates into a gain of information with respect to the simpleGaussian predictor described.\"\n",
    "    \n",
    "    actual = 2d array of indices (Nx1), values are from 0 to D-1.\n",
    "    predicted = 2d array of probabilities (NxD) sum to one along rows.\n",
    "    \"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    return -np.sum(np.log(np.take_along_axis(predicted,actual.astype(int),axis=1)))\n",
    "\n",
    "def MNLPD(actual,predicted):\n",
    "    \"\"\"Mean of the NLPD\"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    return -np.mean(np.log(np.take_along_axis(predicted,actual.astype(int),axis=1)))\n",
    "def GMNLPD(actual,predicted):\n",
    "    \"\"\"Grouped Mean NLPD\n",
    "    This finds the mean of the MNLPDs for all the separate actual groups.\n",
    "    It means that if one large group is well classified it won't swamp the results from\n",
    "    the smaller groups\"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    res = []\n",
    "    for i in np.unique(actual):\n",
    "        res.append(MNLPD(actual[actual[:,0]==i],predicted[actual[:,0]==i,:]))\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN solution (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5655.690000928037"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deep Neural network\n",
    "NLPD(smYtest,smXtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5655913587237136"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean group means.\n",
    "GMNLPD(smYtest,smXtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to regularise the CNN softmax solution to see if it helps (as we regularise our EUE method, so it's fair to try it here too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempX = smXtest+0.01\n",
    "tempX=(tempX.T/np.sum(tempX,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5764.5842905439085"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deep Neural network\n",
    "NLPD(smYtest,tempX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3017689271373074"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean group means.\n",
    "GMNLPD(smYtest,tempX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't help the NLPD but it does help the grouped mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Uncertainty Estimation\n",
    "\n",
    "Using just the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-c4c2b0539d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmXtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmYtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(smXtrain,1)\n",
    "conf = np.zeros([19,19])\n",
    "for p,a in zip(preds,smYtrain[:,0].astype(int)):\n",
    "    conf[p,a]+=1.0\n",
    "conf+=0.01\n",
    "conf = (conf.T/np.sum(conf,1)).T\n",
    "#plt.matshow(conf)\n",
    "#plt.colorbar()\n",
    "newprobs = np.zeros_like(smXtest)\n",
    "preds = np.argmax(smXtest,1)\n",
    "for i,p in enumerate(preds):\n",
    "    newprobs[i,:] = conf[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5263.5786170675565"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLPD(smYtest,newprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.227997621593037"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMNLPD(smYtest,newprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probabilities to build a sort of \"probability confusion matrix\" (this might have a name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f3fa1eb9df0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(conf.T)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f3fa1f59ac0>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(conf)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.zeros([19,19])\n",
    "#count = np.zeros([19,19])\n",
    "for x,a in zip(smXtrain,smYtrain[:,0].astype(int)):\n",
    "    conf[:,a]+=x\n",
    "    #count[:,a]+=1\n",
    "    \n",
    "conf = (conf.T/np.sum(conf,1)).T\n",
    "#conf = (conf/np.sum(conf,0))\n",
    "\n",
    "#conf = conf/count\n",
    "#conf[conf>1]=1\n",
    "#plt.matshow(conf)\n",
    "#plt.colorbar()\n",
    "newprobs = np.zeros_like(smXtest)\n",
    "preds = np.argmax(smXtest,1)\n",
    "for i,p in enumerate(preds):\n",
    "    probs = smXtest[i,:]@conf\n",
    "    #p = np.argmax(probs)\n",
    "    newprobs[i,:] = conf[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5263.5786170675565"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLPD(smYtest,newprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.227997621593037"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMNLPD(smYtest,newprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the NLPD is reduced to 5281 from 5655 (6.6% less).\n",
    "\n",
    "I think the benefit might be stronger in the less common speicies, it might be interesting to weight them evenly (at the moment as you mentioned it's not been rebalanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
