{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GPy\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True, precision=10)\n",
    "from paramz.transformations import Logexp\n",
    "import itertools\n",
    "\n",
    "def monte_carlo_int(m,steps=1500):\n",
    "    \"\"\"\n",
    "    My quick and dirty monte carlo integration.\n",
    "    Returns a mean and the bounds of the 95%\n",
    "    confidence interval (and the number of parameters\n",
    "    it is integrating over).\n",
    "    \"\"\"\n",
    "    m.optimize()\n",
    "    opt = m.optimizer_array[None,:].copy()\n",
    "    nparams = len(m.optimizer_array)\n",
    "    searchwidth = 8\n",
    "    V = searchwidth**nparams\n",
    "    randnos = (np.random.rand(steps,nparams)-0.5)*searchwidth\n",
    "    #randnos[:,2]=0\n",
    "    test_params = randnos+opt.repeat(steps,0)[:]\n",
    "    tot = []\n",
    "    for i in range(steps):\n",
    "        tot.append(np.exp(-m._objective(test_params[i,:])))\n",
    "    mean = V*np.mean(tot)\n",
    "    ste = V*np.std(tot)/np.sqrt(len(tot))\n",
    "    return mean,mean-ste*1.96,mean+ste*1.96,nparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at $p(y|M_i, X)$ for two models ($i=1, i=2$). In each we integrate over all the hyperparemeters. The difference is how wide the Normal prior is over the lengthscale. I decided to use this as a way of modifying the model complexity as fixing parameters etc seems a bit less well defined.\n",
    "\n",
    "We can see a tiny increase in the log likelihood, from -107.4 to -106.7, this was barely perceptible to be honest, I was sort of expecting a bigger increase in the marginal likelihood (over hyperparameters).\n",
    "\n",
    "(note that as X and Y will change, you'll need to insert a new number into the prior if you want to run this again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(0,40,1)[:,None]\n",
    "Y = 10*(np.sin(X/5)+np.random.randn(X.shape[0],X.shape[1])*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===lengthscale unfixed===\n",
      "\n",
      "Name : GP regression\n",
      "Objective : 112.223201473\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |          value  |  constraints  |    priors   \n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |  95.4499726575  |      +ve      |             \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |  8.58564552841  |               |  N(8.5, 2.5)\n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  8.51925249128  |      +ve      |             \n",
      "[-107.4569539934 -107.4701229028 -107.4439562525]\n",
      "==lengthscale fixed===\n",
      "\n",
      "Name : GP regression\n",
      "Objective : 108.496101584\n",
      "Number of Parameters : 3\n",
      "Number of Optimization Parameters : 3\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1mGP_regression.         \u001b[0;0m  |          value  |  constraints  |     priors   \n",
      "  \u001b[1mrbf.variance           \u001b[0;0m  |  92.2117800071  |      +ve      |              \n",
      "  \u001b[1mrbf.lengthscale        \u001b[0;0m  |  8.47015995706  |               |  N(8.5, 0.06)\n",
      "  \u001b[1mGaussian_noise.variance\u001b[0;0m  |  8.52882305365  |      +ve      |              \n",
      "[-106.7086604126 -106.7865225322 -106.636425308 ]\n"
     ]
    }
   ],
   "source": [
    "for fixed in [False,True]:\n",
    "    k = GPy.kern.RBF(1)\n",
    "    #create model and optimise\n",
    "    m2 = GPy.models.GPRegression(X,Y,k)\n",
    "    if fixed:\n",
    "        m2.kern.lengthscale.unconstrain()\n",
    "        m2.kern.lengthscale.set_prior(GPy.priors.Gaussian(8.47,0.06))\n",
    "    else:\n",
    "        m2.kern.lengthscale.unconstrain()\n",
    "        m2.kern.lengthscale.set_prior(GPy.priors.Gaussian(8.47,2.5))\n",
    "    m2.optimize()\n",
    "    \n",
    "    if fixed:\n",
    "        print \"==lengthscale fixed===\"\n",
    "    else:\n",
    "        print \"===lengthscale unfixed===\"\n",
    "    print m2\n",
    "    best = m2.param_array[:].copy()    \n",
    "    m2.optimize()\n",
    "    print np.log(monte_carlo_int(m2,30000))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "My main concern is now that comparing the different models (with either the data combined or independently fitting, with the offset parameter in the combined model etc) might not really be very robust using this method.\n",
    "\n",
    "The LOO x-validation method largely will incorporate model complexity automatically (if you're over fitting, then the left-out test data point will be poorly fitted). Also I need not worry about trying to integrate something that might be quite tricky/funny shape.\n",
    "\n",
    "The only downside is the problem that Alan pointed out - I'm not really accounting for the full uncertainty as I'm just using the MAP estimate. After spending far too much time on this I think that this is probably an acceptable compromise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
